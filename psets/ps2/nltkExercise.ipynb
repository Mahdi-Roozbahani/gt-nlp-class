{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords, state_union, gutenberg, wordnet, movie_reviews\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# this enables you to create inline plots in the notebook \n",
    "%pylab inline\n",
    "\n",
    "# shift + tab will give help on functiom\n",
    "# ctrl + enter will run inline cell run\n",
    "# shift + enter will run the cell and create new next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Excersise codes go here\n",
    "\n",
    "#some examples of word,sentence, lexicon (words and their means), and corpores tokenizing\n",
    "sentence = \"hello there, how are you doing today? the weather is great and Python is awesome. The sky is pinkish-blue. You should not eat cardboard.\"\n",
    "## Some sample codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(sent_tokenize(sentence));\n",
    "wt = word_tokenize\n",
    "#print(word_tokenize(sentence)); # in python we don't need to do new to create a new object. For instance here word_tokenize is a class that we called.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i in word_tokenize(sentence):\n",
    "#    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "#Stop words\n",
    "example_sen = \"This is an example showing off stop word filtration.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#print(stop_words) # showing the common stop words\n",
    "\n",
    "### one wat to filter off a sentence from stop words\n",
    "\n",
    "#words = word_tokenize(example_sen)\n",
    "#filtered_sentence = []\n",
    "#for w in words:\n",
    "#    if w not in stop_words:\n",
    "#        filtered_sentence.append(w)\n",
    "#print(filtered_sentence)\n",
    "\n",
    "### ALternative way\n",
    "\n",
    "# the first w is just the variable that result will save into this\n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# steming the words (getting the root) \n",
    "# I was taking a rid in the car.\n",
    "# I was riding in the car.\n",
    "\n",
    "ps = PorterStemmer() # creating the object ps from class PorterStemmer\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "    #print(PorterStemmer().stem(w)) # or I can write it like\n",
    "    #print(ps.stem(w))\n",
    "    ps.stem(w)\n",
    "\n",
    "# New example\n",
    "new_text = \"it is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "words = word_tokenize(new_text)\n",
    "w1 = [ps.stem(w) for w in words] # an example for loops inside the list\n",
    "#for w in w1:\n",
    "#    print(w)\n",
    "\n",
    "#for w in words:\n",
    "    #print(ps.stem(w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part of speech tagging ()\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = nltk.PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            #print(words)\n",
    "            tagged = nltk.pos_tag(words) # whether it is adjective verb noun and ...\n",
    "            #print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## chunking (group of something)\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = nltk.PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words) # whether it is adjective verb noun and ...\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?} \"\"\" # searching for RB RBR VB ...\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            #chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## chinking (Removal of something) \n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = nltk.PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words) # whether it is adjective verb noun and ...\n",
    "            \n",
    "            # first part take everything\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+} \n",
    "                                    }<VB.?|IN|DT>+{\"\"\" # remove these things from everything\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            #chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NameEntity recognition\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = nltk.PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words) # whether it is adjective verb noun and ...\n",
    "            \n",
    "            nameEnt = nltk.ne_chunk(tagged, binary = true)\n",
    "            #nameEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lemmatizing (it is like stem but the root word has a meaning)\n",
    "\n",
    "lemmitizaer = WordNetLemmatizer()\n",
    "\n",
    "#print(lemmitizaer.lemmatize(\"better\"))\n",
    "#print(lemmitizaer.lemmatize(\"better\",pos = \"a\")) # I want the adjective stem of it\n",
    "#print(lemmitizaer.lemmatize(\"best\",pos = \"a\"))\n",
    "#print(lemmitizaer.lemmatize(\"run\",pos = \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(nltk.__file__)\n",
    "\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "tok = sent_tokenize(sample)\n",
    "#print(tok[5:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## WordNet\n",
    "\n",
    "syns = wordnet.synsets(\"program\"); # what are the synonyms of program\n",
    "#print(syns[0])\n",
    "#print(syns[0].lemmas()[0].name())\n",
    "\n",
    "# definition\n",
    "#print(syns[0].definition())\n",
    "\n",
    "#examples\n",
    "#print(syns[0].examples())\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        #print(\"l: \",l)\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms(): # get the antonyms of that lemma if there is any\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "#print(set(synonyms))\n",
    "#print(set(antonyms))\n",
    "\n",
    "#symantic\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "#print(w1.wup_similarity(w2)) # compare the similarity of word 1 to word 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set: A collection that contains no duplicate elements. More formally, sets contain no pair of elements e1 and e2 such that e1.equals(e2), and at most one null element. As implied by its name, this interface models the mathematical set abstraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentiment analysis like spam or not spam (Naive Bayes)\n",
    "# There are many reviews from different people. Each review has a label or category of positive or negative\n",
    "# and each review consists of many words. Each word is defined as a feature in our training\n",
    "\n",
    "\n",
    "# words are features (each word is a feature)\n",
    "documents = [(list(movie_reviews.words(fileid)),category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "# the other way to write above\n",
    "\n",
    "#documents = []\n",
    "#for category in movie_reviews.categories():\n",
    "#    for fileid in movie_reviews.fileids(category):\n",
    "#        documents.append(list(movie_reviews.words(fileid)),category) # document(fileid,category) tuples\n",
    "\n",
    "random.shuffle(documents)\n",
    "#print(documents[3]) # an example of our bag of words\n",
    "#print(movie_reviews.fileids(\"pos\")[0])\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    w = w.lower() # convert everything to a lower case\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "#print(all_words.most_common(15)) # top 15 most common words\n",
    "#print(all_words[\"stupid\"]) # how many times sthe word \"stupid\" has been used\n",
    "\n",
    "word_features = list(all_words.keys())[:3000] # top 3000 words\n",
    "#print(word_features[0])\n",
    "\n",
    "def find_feature(document):\n",
    "    words = set(document) \n",
    "    features = {} # empty dictionary key:value\n",
    "    for w in word_features: # we search through top 3000 words \n",
    "        features[w] = (w in words) # here w is key and value is true or flase whether the word is among top 3000 or not\n",
    "    return features\n",
    "\n",
    "\n",
    "#print((find_feature(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "# in fact by doing the below call, we are creating feature out of each word. Here we are saying each word that is not in top 3000 is false and otherwise is true, and positive and negative would be label of that\n",
    "featuresets = [(find_feature(rev), category) for (rev, category) in documents]\n",
    "\n",
    "training_set = featuresets[:1900]\n",
    "testing_test = featuresets[1900:]\n",
    "\n",
    "print(training_set[0]) # the first document\n",
    "# posterior = prior occurences * likelihood /evidence\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive Bayes Algo accuracy:\", (nltk.classify.accuracy(classifier, testing_test))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
